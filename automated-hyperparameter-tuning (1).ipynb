{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.ytimg.com/vi/dA_x2xHTYQE/maxresdefault.jpg)\n",
    "\n",
    "<font size='5' color='blue' align = 'center'>Table of Contents</font> \n",
    "<font size='3' color='purple'>\n",
    "1. [Introduction](#1)\n",
    "1. [**Manual Search**](#2)\n",
    "1. [**Grid Search**](#3)\n",
    "1. [**Random Search**](#4)\n",
    "1. [Automated Hyperparameter Tuning](#5)\n",
    "    1. [Bayesian Optimization using **HyperOpt**](#51)\n",
    "    1. [Genetic Algorithms using **TPOT**](#52)\n",
    "    1. [Artificial Neural Networks (ANNs) Tuning](#53)\n",
    "1. [**Optuna**](#6)    \n",
    "1. [**Tune**](#7)    \n",
    "1. [**Sherpa**](#8)    \n",
    "1. [Conclusion](#9)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id=\"1\"></a> <br>\n",
    "\n",
    "**Hyperparameter tuning** is choosing a set of optimal hyperparameters for a learning algorithm.\n",
    "\n",
    "**What is a hyperparameter?\n",
    "\n",
    "**A hyperparameter is a parameter whose value is set before the learning process begins.**\n",
    "\n",
    "Some examples of hyperparameters include penalty in logistic regression and loss in stochastic gradient descent.\n",
    "\n",
    "In sklearn, hyperparameters are passed in as arguments to the constructor of the model classes.\n",
    "\n",
    "Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n",
    "\n",
    "It is possible and recommended to search the hyper-parameter space for the best Cross-validation i.e evaluating estimator performance score.\n",
    "\n",
    "Any parameter provided when constructing an estimator may be optimized in this manner. Specifically, to find the names and current values for all parameters for a given estimator, we can use the following method\n",
    "\n",
    "estimator.get_params()\n",
    "\n",
    "A search consists of:\n",
    "\n",
    "* an estimator (regressor or classifier such as sklearn.svm.SVC());\n",
    "* a parameter space;\n",
    "* a method for searching or sampling candidates;\n",
    "* a cross-validation scheme;\n",
    "* a score function.\n",
    "\n",
    "Some models allow for specialized, efficient parameter search strategies, outlined below.\n",
    "\n",
    "Two generic approaches to sampling search candidates are provided in scikit-learn:\n",
    "![](https://developer.qualcomm.com/sites/default/files/attachments/learning_resources_03-05.png)\n",
    "**GridSearchCV** :For given values, GridSearchCV exhaustively considers all parameter combinations. The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter.\n",
    "For instance, the following param_grid specifies that it has one grid to be explored that is a linear kernel with alpha values in [0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0009], and 'max_iter' i.e maximum 10000 iterations.\n",
    "\n",
    "param_grid = {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0009],'max_iter':[10000]}\n",
    "\n",
    "**RandomizedSearchCV**: It can sample a given number of candidates from a parameter space with a specified distribution.\n",
    "After describing these tools we detail best practice applicable to both approaches.\n",
    "\n",
    "Note that it is common that a small subset of those parameters can have a large impact on the predictive or computation performance of the model while others can be left to their default values. It is recommend to read the docstring of the estimator class to get a finer understanding of their expected behavior.\n",
    "\n",
    "I think it is enough of the theory .Now lets jump into practice.\n",
    "\n",
    "To perform Hyperparameters Optimization in Python, we will use Credit Card Fraud Detection Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv(r'C:\\Users\\iyers\\Classes\\PluralSight\\ML_Content\\Optimization_in_ML\\data\\creditcard\\creditcard.csv',na_values = '#NAME?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['V17', 'V9', 'V6', 'V12']]\n",
    "Y = df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.30,random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Manual Search <a id=\"2\"></a> <br>\n",
    "We will use a Random Forest Classifier as our model to optimize.Random Forest models are formed by a large number of uncorrelated decision trees, which joint together constitute an ensemble. In Random Forest, each decision tree makes its own prediction and the overall model output is selected to be the prediction which appeared most frequently.\n",
    "\n",
    "We can now start by calculating our base model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85289    10]\n",
      " [   34   110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85299\n",
      "           1       0.92      0.76      0.83       144\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.96      0.88      0.92     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier(random_state= 101).fit(X_Train,Y_Train)\n",
    "predictionforest = model.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionforest))\n",
    "print(classification_report(Y_Test,predictionforest))\n",
    "acc1 = accuracy_score(Y_Test,predictionforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Manual Search, we choose some model hyperparameters based on our judgment/experience. We then train the model, evaluate its accuracy and start the process again. This loop is repeated until a satisfactory accuracy is scored.\n",
    "\n",
    "The main parameters used by a Random Forest Classifier are:\n",
    "\n",
    "* criterion = the function used to evaluate the quality of a split.\n",
    "* max_depth = maximum number of levels allowed in each tree.\n",
    "* max_features = maximum number of features considered when splitting a node.\n",
    "* min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n",
    "* min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n",
    "* n_estimators = number of trees in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85288    11]\n",
      " [   41   103]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85299\n",
      "           1       0.90      0.72      0.80       144\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.86      0.90     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=10, random_state= 101).fit(X_Train,Y_Train)\n",
    "predictionforest = model.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionforest))\n",
    "print(classification_report(Y_Test,predictionforest))\n",
    "acc2 = accuracy_score(Y_Test,predictionforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Search <a id=\"3\"></a> <br>\n",
    "\n",
    "In Random Search, we create a grid of hyperparameters and train/test our model on just some random combination of these hyperparameters. In this example, I additionally decided to perform Cross-Validation on the training set.\n",
    "\n",
    "When performing Machine Learning tasks, we generally divide our dataset in training and test sets. This is done so that to test our model after having trained it (in this way we can check itâ€™s performances when working with unseen data). When using Cross-Validation, we divide our training set into N other partitions to make sure our model is not overfitting our data.\n",
    "\n",
    "One of the most common used Cross-Validation methods is K-Fold Validation. In K-Fold, we divide our training set into N partitions and then iteratively train our model using N-1 partitions and test it with the left-over partition (at each iteration we change the left-over partition). Once having trained N times the model we then average the training results obtained in each iteration to obtain our overall training performance results.\n",
    "\n",
    "Using Cross-Validation when implementing Hyperparameters optimization can be really important. In this way, we might avoid using some Hyperparameters which works really good on the training data but not so good with the test data.\n",
    "We can now start implementing Random Search by first defying a grid of hyperparameters which will be randomly sampled when calling RandomizedSearchCV()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;entropy&#x27;, &#x27;gini&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [2],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [4, 6, 8],\n",
       "                                        &#x27;min_samples_split&#x27;: [5, 7, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [20]},\n",
       "                   random_state=101, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;entropy&#x27;, &#x27;gini&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [2],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [4, 6, 8],\n",
       "                                        &#x27;min_samples_split&#x27;: [5, 7, 10],\n",
       "                                        &#x27;n_estimators&#x27;: [20]},\n",
       "                   random_state=101, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=4, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "                   param_distributions={'criterion': ['entropy', 'gini'],\n",
       "                                        'max_depth': [2],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [4, 6, 8],\n",
       "                                        'min_samples_split': [5, 7, 10],\n",
       "                                        'n_estimators': [20]},\n",
       "                   random_state=101, verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "random_search = {'criterion': ['entropy', 'gini'],\n",
    "               'max_depth': [2],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'min_samples_leaf': [4, 6, 8],\n",
    "               'min_samples_split': [5, 7,10],\n",
    "               'n_estimators': [20]}\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "model = RandomizedSearchCV(estimator = clf, param_distributions = random_search, n_iter = 10, \n",
    "                               cv = 4, verbose= 1, random_state= 101, n_jobs = -1)\n",
    "model.fit(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained our model, we can then visualize how changing some of its Hyperparameters can affect the overall model accuracy. In this case, I decided to observe how changing the number of estimators and the criterion can affect our Random Forest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='param_criterion', ylabel='param_n_estimators'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAHACAYAAAARCkpCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4o0lEQVR4nO3deXRV1f3//9eFzARCmDIgJGEOkDBIgTAsQMGAiAwVMaIBBVpUZBI+bdZXZLJGlwZpwTIL4oCCDFIFZBCsQJSZgjKGIQUSoAiESAg02b8/+vPqNQkkN+cSkvt8uM5a3H322ed9Y1Pe7vfe59iMMUYAAAAWKVfSAQAAgLKF5AIAAFiK5AIAAFiK5AIAAFiK5AIAAFiK5AIAAFiK5AIAAFiK5AIAAFiK5AIAAFiK5AIAAFiK5AIAyrB//vOf6tWrl0JDQ2Wz2bRq1SqX3m/SpEmy2WwOR6NGjYo15tKlS9W8eXP5+fkpLCxMb7755h2v2bNnj7p166bKlSuratWq+sMf/qDMzEyHPps2bVK7du1UsWJFBQcH609/+pP++9//2s/fuHFDgwcPVlRUlDw8PNSnT59ifY/CKIl7ugLJBQCUYT/99JOaNWumd955567ds0mTJkpLS7MfW7duvW1/m82mU6dO5Xtu7dq1GjhwoIYPH66DBw/q73//u95++23NnDmzwPHOnTunrl27ql69evruu++0bt06ff/99xo8eLC9z/79+/Xwww+re/fu2rt3rz755BOtXr1af/7zn+19cnJy5Ovrq5EjR6pr165F+hk4qyTu6RIGAOAWJJmVK1c6tN24ccO89NJLJjQ01Pj5+ZnWrVubzZs3O32PiRMnmmbNmhU5rpMnT+Z7Li4uzjz22GMObX/729/MfffdZ3Jzc/O9Zs6cOaZGjRomJyfH3vavf/3LSDLHjh0zxhiTkJBgWrVq5XDd6tWrjY+Pj8nIyMgz5qBBg0zv3r3zvd+qVatMixYtjLe3t4mIiDCTJk0yt27dKujrFtrt7nmvY+YCANzYiBEjlJycrI8//lj/+te/1L9/f3Xv3l3Hjh1zesxjx44pNDRUderU0cCBA5Wamur0WNnZ2fLx8XFo8/X11ZkzZ3T69OkCr/Hy8lK5cuUcrpFkn0UpaNwbN25o9+7dhY7vm2++UXx8vEaNGqUffvhBc+bM0aJFi/SXv/yl0GOURSQXAOCmUlNTtXDhQi1btkwdO3ZU3bp1NW7cOHXo0EELFy50asw2bdpo0aJFWrdunWbNmqWTJ0+qY8eOunbtmlPjxcbGasWKFdq0aZNyc3N19OhRJSUlSZLS0tLyveaBBx5Qenq63nzzTd28eVOXL1+2lzt+viY2Nlbbt2/XkiVLlJOTo7Nnz2rKlCm3HTc/kydP1p///GcNGjRIderUUbdu3TR16lTNmTPHqe9bVpBcAICbOnDggHJyctSgQQP5+/vbj6+//lopKSmSpMOHD+dZoPnb49frFHr06KH+/fsrOjpasbGxWrNmja5cuaKlS5c69Pn1/aT/rdP4+XOTJk3sfYcNG6YRI0bokUcekZeXl9q2basnnnhCkhxmJn6tSZMmeu+995SUlCQ/Pz8FBwcrIiJCQUFB9mseeughvfnmmxo+fLi8vb3VoEEDPfzww7cdNz/79+/XlClTHL7PsGHDlJaWpuvXr0uS2rZte9ufX3BwcKHvV1p4lHQAAICSkZmZqfLly2v37t0qX768w7mf/9KvU6eODh06dNtxqlatWuC5ypUrq0GDBjp+/Li9bf78+crKyrJ/rl+/vtasWaOaNWtKkjw9Pe3nbDab3njjDb322mtKT09X9erVtWnTJntsBXnyySf15JNP6vz586pQoYJsNpumTZvmcM3YsWM1ZswYpaWlKTAwUKdOnVJCQsJtx/2tzMxMTZ48Wf369ctz7ueyyyeffOLwfX/Lw6Ps/VVc9r4RAKBQWrRooZycHF24cEEdO3bMt4+Xl1extpJmZmYqJSVFTz/9tL3t5yTi18LCwhQeHl7gOOXLl7dft2TJEsXExKh69ep3vH9QUJAk6d1335WPj4+6devmcN5msyk0NNQ+bq1atdSyZcs7jvuzli1b6siRI6pXr16BfcLCwgo9XllBcgEAZVhmZqbDrMHJkye1b98+ValSRQ0aNNDAgQMVHx+vpKQktWjRQhcvXtSmTZsUHR2tnj17Fvl+48aNU69evRQWFqZz585p4sSJKl++vOLi4pyK/z//+Y8+/fRTde7cWTdu3LCvEfn666/tfXbs2KH4+Hht2rTJnoDMnDlT7dq1k7+/vzZs2KDx48fr9ddfV+XKle3Xvfnmm+revbvKlSunFStW6PXXX9fSpUsdZnF++OEH3bx5Uz/++KOuXbumffv2SZKaN28uSXrllVf0yCOPqHbt2nrsscdUrlw57d+/XwcPHtSrr77q1He+0z1LhZLergIAcJ3NmzcbSXmOQYMGGWOMuXnzpnnllVdMeHi48fT0NCEhIaZv377mX//6l1P3GzBggAkJCTFeXl6mZs2aZsCAAeb48eO3vUa32Yp68eJF07ZtW1OhQgXj5+dnHnzwQfPtt9/m+x1/PcbTTz9tqlSpYry8vEx0dLRZvHhxnrG7dOliAgICjI+Pj2nTpo1Zs2ZNnj5hYWH5/vx+bd26daZdu3bG19fXVKpUybRu3drMnTv3tt/5dgpzz3udzRhj7npGAwAAyix2iwAAAEuRXAAAAEuRXAAAAEuVyd0iHl55tzkBkLLOfVPSIQD3HM9qhX+uhbNu/eeEJePcjVitUCaTCwAA7im5OSUdwV1FWQQAAFiKmQsAAFzN5JZ0BHcVyQUAAK6W617JBWURAABgKWYuAABwMUNZBAAAWIqyCAAAgPOYuQAAwNUoiwAAAEvxEC0AAADnMXMBAICrURYBAACWcrPdIiQXAAC4mLs954I1FwAAwFLMXAAA4GqURQAAgKUoiwAAADiPmQsAAFzNzR6iRXIBAICrURYBAABwHjMXAAC4GrtFAACApSiLAAAAOI+ZCwAAXI2yCAAAsJIxbEUFAABWYs0FAACA85i5AADA1VhzAQAALEVZBAAAwHnMXAAA4Gq8uAwAAFiKsggAACjtwsPDZbPZ8hwvvPBCgdcsW7ZMjRo1ko+Pj6KiorRmzRqn7k1yAQCAq+XmWnMUwc6dO5WWlmY/NmzYIEnq379/vv23b9+uuLg4DRkyRHv37lWfPn3Up08fHTx4sMhf12aMMUW+6h7n4VWzpEMA7klZ574p6RCAe45ntTouv8eN5CWWjOMTE+f0taNHj9bnn3+uY8eOyWaz5Tk/YMAA/fTTT/r888/tbW3btlXz5s01e/bsIt2LmQsAAMq4mzdv6oMPPtCzzz6bb2IhScnJyeratatDW2xsrJKTk4t8PxZ0AgDgahY9RCs7O1vZ2dkObd7e3vL29r7tdatWrdKVK1c0ePDgAvukp6crKCjIoS0oKEjp6elFjpOZCwAAXM2iNReJiYkKCAhwOBITE+94+wULFqhHjx4KDQ29C1+WmQsAAFzOqreiJiQkaOzYsQ5td5q1OH36tDZu3KgVK1bctl9wcLDOnz/v0Hb+/HkFBwcXOU5mLgAAKCW8vb1VqVIlh+NOycXChQtVo0YN9ezZ87b9YmJitGnTJoe2DRs2KCYmpshxMnMBAICrldCLy3Jzc7Vw4UINGjRIHh6Of+XHx8erZs2a9rLKqFGj1KlTJyUlJalnz576+OOPtWvXLs2dO7fI92XmAgAAVzO51hxFtHHjRqWmpurZZ5/Ncy41NVVpaWn2z+3atdNHH32kuXPnqlmzZvr000+1atUqNW3atMj35TkXgBvhORdAXnfjORdZm+dbMo5vl6GWjONqlEUAAHC1EiqLlBSSCwAAXI0XlwEAADiPmQsAAFyNsggAALAUZREAAADnMXMBAICrURYBAACWIrkAAACWYs0FAACA85i5AADA1SiLAAAAS1EWAQAAcB4zFwAAuBplEQAAYCnKIgAAAM5j5gIAAFejLAIAACzlZskFZREAAGApZi4AAHA1Y0o6gruK5AIAAFdzs7IIyQUAAK7mZskFay4AAIClmLkAAMDV3OwhWiQXAAC4GmURAAAA5zFzAQCAq7EVFQAAWIqyCAAAgPOYuQAAwNXcbOaC5AIAAFdzs62olEUAAIClmLkAAMDFTC67RQAAgJVYcwEAACzFmgsAAADnMXMBAICrseYCAABYys3WXFAWAQAAlmLmAgAAV3OzmQuSCwAAXM3N3opKWQQAAFiKmQsAAFyNsggAALAUW1EBAICleEInAACA85i5AADA1SiLAAAAKxk3W9BJWQQAAFiKmQsAAFyNsggAALAUu0UAAACcx8wFAACuRlkEAABYit0iAAAAzmPmAgAAV6MsAgAALOVmu0VILgAAcDU3m7lgzQUAALAUMxcAALiYu71bhOQCAABXoywCAADgPGYuAABwNTebuSC5AADA1dxsKyplEQAAYClmLgAAcDXKIgAAwErGzZILyiIAAMBSzFwAAOBqbjZzQXIBAICr8YROAABgKTebuWDNBQAAZdTZs2f11FNPqWrVqvL19VVUVJR27dpVYP8tW7bIZrPlOdLT04t0X6dmLrKysmSMkZ+fnyTp9OnTWrlypRo3bqyHHnrImSEBACi7SmDm4vLly2rfvr26dOmitWvXqnr16jp27JgCAwPveO2RI0dUqVIl++caNWoU6d5OJRe9e/dWv379NHz4cF25ckVt2rSRp6en/vOf/2jatGl67rnnnBkWAIAyyZi7n1y88cYbqlWrlhYuXGhvi4iIKNS1NWrUUOXKlZ2+t1NlkT179qhjx46SpE8//VRBQUE6ffq0Fi9erL/97W9OBwMAAKyxevVqtWrVSv3791eNGjXUokULzZs3r1DXNm/eXCEhIerWrZu2bdtW5Hs7lVxcv35dFStWlCStX79e/fr1U7ly5dS2bVudPn3amSEBACi7co0lR3Z2tjIyMhyO7OzsfG954sQJzZo1S/Xr19eXX36p5557TiNHjtR7771XYJghISGaPXu2li9fruXLl6tWrVrq3Lmz9uzZU6SvazNOzNVER0dr6NCh6tu3r5o2bap169YpJiZGu3fvVs+ePYu88MNqHl41S/T+wL0q69w3JR0CcM/xrFbH5ffIGNLNknGm1WqvyZMnO7RNnDhRkyZNytPXy8tLrVq10vbt2+1tI0eO1M6dO5WcnFzoe3bq1Em1a9fW+++/X+hrnJq5eOWVVzRu3DiFh4erTZs2iomJkfS/WYwWLVo4MyQAALiDhIQEXb161eFISEjIt29ISIgaN27s0BYZGanU1NQi3bN169Y6fvx4ka5xakHnY489pg4dOigtLU3NmjWztz/44IPq27evM0MCAFBmWfVuEW9vb3l7exeqb/v27XXkyBGHtqNHjyosLKxI99y3b59CQkKKdE2Rk4tbt27J19dX+/btyzNL0bp166IOBwBA2VcCW1HHjBmjdu3a6bXXXtPjjz+uHTt2aO7cuZo7d669T0JCgs6ePavFixdLkqZPn66IiAg1adJEN27c0Pz58/XVV19p/fr1Rbp3kZMLT09P1a5dWzk5OUW9FAAA3CW/+93vtHLlSiUkJGjKlCmKiIjQ9OnTNXDgQHuftLQ0hzLJzZs39dJLL+ns2bPy8/NTdHS0Nm7cqC5duhTp3k4t6FywYIFWrFih999/X1WqVCnq5S7Hgk4gfyzoBPK6Gws6rz79oCXjBLy/yZJxXM2pNRczZ87U8ePHFRoaqrCwMFWoUMHhfFG3rAAAUJZZteaitHAquejTp4/FYQAAUIaRXNzZxIkTrY4DAACUEcV65fru3bt16NAhSVKTJk14xgUAAPnJLekA7i6nkosLFy7oiSee0JYtW+wvNrly5Yq6dOmijz/+WNWrV7cyRgAASjV3W3Ph1BM6X3zxRV27dk3ff/+9fvzxR/344486ePCgMjIyNHLkSKtjBAAApYhTMxfr1q3Txo0bFRkZaW9r3Lix3nnnHT300EOWBQcAQJlAWeTOcnNz5enpmafd09NTublu9hMEAOAOKIsUwgMPPKBRo0bp3Llz9razZ89qzJgxevBBax4UAgAASienkouZM2cqIyND4eHhqlu3rurWrauIiAhlZGRoxowZVscIAEDplmvRUUo4VRapVauW9uzZo40bN+rw4cOS/vca165du1oaHAAAZYEpRYmBFZxKLhYvXqwBAwaoW7du6tatm7395s2b+vjjjxUfH29ZgAAAoHRxqizyzDPP6OrVq3nar127pmeeeabYQQEAUKZQFrkzY4xsNlue9jNnziggIKDYQQEAUJZQFrmNFi1ayGazyWaz6cEHH5SHxy+X5+Tk6OTJk+revbvlQQIAUKqRXBTs57eh7tu3T7GxsfL397ef8/LyUnh4uH7/+99bGiAAAChdipRc/Pw21PDwcA0YMEA+Pj4uCQoAgLKEskghDBo0yOo4AAAos0guCiEnJ0dvv/22li5dqtTUVN28edPh/I8//mhJcAAAoPRxaivq5MmTNW3aNA0YMEBXr17V2LFj1a9fP5UrV06TJk2yOEQAAEo3k2vNUVo4lVx8+OGHmjdvnl566SV5eHgoLi5O8+fP1yuvvKJvv/3W6hgBACjdjM2ao5RwKrlIT09XVFSUJMnf39/+QK1HHnlEX3zxhXXRAQCAUsep5OK+++5TWlqaJKlu3bpav369JGnnzp3y9va2LjoAAMoAyiKF0LdvX23atEmS9OKLL2rChAmqX7++4uPj9eyzz1oaIAAApZ3JtVlylBY2Y4wp7iDJyclKTk5W/fr11atXLyviKhYPr5olHQJwT8o6901JhwDcczyr1XH5PdI6dLFknJCtmy0Zx9Wc2or6WzExMYqJibFiKAAAypzSVNKwgtPJxblz57R161ZduHBBubmOP7WRI0cWOzAAAMoKU4p2eljBqeRi0aJF+uMf/ygvLy9VrVrV4Q2pNpuN5AIAgF9h5qIQJkyYoFdeeUUJCQkqV86pNaEAAKCMciq5uH79up544gkSCwAACqE07fSwglPZwZAhQ7Rs2TKrYwEAoEwyxpqjtHBqK2pOTo4eeeQRZWVlKSoqSp6eng7np02bZlmAzmArKpA/tqICed2NraiprR60ZJzauzZZMo6rOVUWSUxM1JdffqmGDRtKUp4FnQAA4BfuVhZxKrlISkrSu+++q8GDB1scDgAAZY+7JRdOrbnw9vZW+/btrY4FAACUAU4lF6NGjdKMGTOsjgUAgDLJ3RZ0OlUW2bFjh7766it9/vnnatKkSZ4FnStWrLAkOAAAygJ3K4s4lVxUrlxZ/fr1szoWAABQBjiVXCxcuNDqOAAAKLN4twgAALAU7xYpQMuWLbVp0yYFBgaqRYsWt32exZ49eywJDgCAsiCXmYv89e7dW97e3vY/87AsAACQH6ce/32v4/HfQP54/DeQ1914/PeRRj0sGafh4bWWjONqTj3nok6dOrp06VKe9itXrqhOHdf/SwIAoDQxuTZLjtLCqeTi1KlTysnJydOenZ2tM2fOFDsoAABQehVpt8jq1avtf/7yyy8VEBBg/5yTk6NNmzYpIiLCuugAACgDyt4ChNsrUnLRp08fSf978+mgQYMcznl6eio8PFxJSUmWBQcAQFlQmkoaVihScpGb+7+NuhEREdq5c6eqVavmkqAAAEDp5dRDtE6ePJmn7cqVK6pcuXJx4wEAoMxxt+dcOLWg84033tAnn3xi/9y/f39VqVJFNWvW1P79+y0LDgCAssAYmyVHaeFUcjF79mzVqlVLkrRhwwZt3LhR69atU48ePTR+/HhLAwQAAKWLU2WR9PR0e3Lx+eef6/HHH9dDDz2k8PBwtWnTxtIAAQAo7dgtUgiBgYH697//rVq1amndunV69dVXJUnGmHyff3EnO3bsUHJystLT0yVJwcHBiomJUevWrZ0JDwCAe4q7rblwKrno16+fnnzySdWvX1+XLl1Sjx7/e6zp3r17Va9evUKPc+HCBf3+97/Xtm3bVLt2bQUFBUmSzp8/rzFjxqh9+/Zavny5atSo4UyYAADcE0rTegkrOLXm4u2339aIESPUuHFjbdiwQf7+/pKktLQ0Pf/884Ue5/nnn1dOTo4OHTqkU6dO6bvvvtN3332nU6dO6dChQ8rNzdULL7zgTIgAAKCElOiLyypWrKh//vOfatGiRb7nd+/erc6dO+vatWtFGpcXlwH548VlQF5348Vle2r1tmSclv/+zJJxXM2pmQtJev/999WhQweFhobq9OnTkqTp06frs88K/8W9vb2VkZFR4Plr167ZX/MOAEBplWtslhylhVPJxaxZszR27Fj16NFDV65csS/irFy5sqZPn17ocQYMGKBBgwZp5cqVDklGRkaGVq5cqWeeeUZxcXG3HSM7O1sZGRkORxl8izwAAKWGU8nFjBkzNG/ePP2///f/VL58eXt7q1atdODAgUKPM23aNPXo0UNPPPGEAgMD5evrK19fXwUGBuqJJ55Qjx499NZbb912jMTERAUEBDgcJrdoZRQAAFzJ3R6i5dSaC19fXx0+fFhhYWGqWLGi9u/frzp16ujYsWOKjo5WVlZWkcbLyMjQ7t27Hbai3n///apUqdIdr83OzlZ2drZDW2DVRrLZSs+/BOBuYc0FkNfdWHPxXWg/S8Zpc26FJeO4mlNbUSMiIrRv3z6FhYU5tK9bt06RkZFFHq9SpUrq0qWLM6HI29s7z7oMEgsAAEqOU8nF2LFj9cILL+jGjRsyxmjHjh1asmSJEhMTNX/+/CKNlZWVpd27d6tKlSpq3Lixw7kbN25o6dKlio+PdyZMAADuCe62EtDpragffvihJk2apJSUFElSaGioJk+erCFDhhR6jKNHj+qhhx5SamqqbDabOnTooCVLlig0NFTS/x6mFRoaWuSnfrIVFcgfZREgr7tRFtke8ntLxmmXttyScVzN6a2oAwcO1LFjx5SZman09HSdOXMmT2Kxbdu2POshfu1Pf/qTmjZtqgsXLujIkSOqWLGiOnTooNTUVGfDAgAAJczp5OJnfn5+BT6eu0ePHjp79myB127fvl2JiYmqVq2a6tWrp3/84x+KjY1Vx44ddeLEieKGBgDAPcHddosUO7m4nTtVXLKysuTh8cuyD5vNplmzZqlXr17q1KmTjh496srwAAC4K3ItOkoLpxZ0WqVRo0batWtXnh0mM2fOlCQ9+uijJREWAACWMio9sw5WcOnMxZ307dtXS5YsyffczJkzFRcXx9M2AQAoZVz64rJfP2DrbmK3CJA/dosAed2N3SJbgvpbMk7n88ssGcfVXFoW4WFWAABIuZRFrENJAwAA9+PS5OLatWt3vSQCAMC9xshmyVFUZ8+e1VNPPaWqVavK19dXUVFR2rVr122v2bJli1q2bClvb2/Vq1dPixYtKvJ9nUouzp8/r6efflqhoaHy8PBQ+fLlHQ4AAPCLktiKevnyZbVv316enp5au3atfvjhByUlJSkwMLDAa06ePKmePXuqS5cu2rdvn0aPHq2hQ4fqyy+/LNK9nVpzMXjwYKWmpmrChAkKCQlhbQUAAPeYN954Q7Vq1dLChQvtbREREbe9Zvbs2YqIiFBSUpIkKTIyUlu3btXbb7+t2NjYQt/bqeRi69at+uabb9S8eXNnLgcAwK2UxHMuVq9erdjYWPXv319ff/21atasqeeff17Dhg0r8Jrk5GR17drVoS02NlajR48u0r2dKovUqlWLxZoAABSSVWWR7OxsZWRkOBwFvcPrxIkTmjVrlurXr68vv/xSzz33nEaOHKn33nuvwDjT09MVFBTk0BYUFKSMjAxlZWUV+vs6lVxMnz5df/7zn3Xq1ClnLgcAAE5ITExUQECAw5GYmJhv39zcXLVs2VKvvfaaWrRooT/84Q8aNmyYZs+e7fI4nSqLDBgwQNevX1fdunXl5+cnT09Ph/M//vijJcEBAFAWWPVekISEBI0dO9ahzdvbO9++ISEhaty4sUNbZGSkli8v+LXtwcHBOn/+vEPb+fPnValSJfn6+hY6TqeSi+nTpztzGQAAbsmqNRfe3t4FJhO/1b59ex05csSh7ejRowoLCyvwmpiYGK1Zs8ahbcOGDYqJiSlSnE4lF4MGDSpUv9dff13Dhw9X5cqVnbkNAABlQm4JbKocM2aM2rVrp9dee02PP/64duzYoblz52ru3Ln2PgkJCTp79qwWL14sSRo+fLhmzpyp//u//9Ozzz6rr776SkuXLtUXX3xRpHu79CFar732GiUSAABKwO9+9zutXLlSS5YsUdOmTTV16lRNnz5dAwcOtPdJS0tTamqq/XNERIS++OILbdiwQc2aNVNSUpLmz59fpG2oEi8uA9wKLy4D8robLy77LPhJS8bpnf6RJeO4mktfXAYAACR3e3iDS8siAADA/TBzAQCAi1m1FbW0ILkAAMDFct3sHVwuLYt07NixSA/dAAAApV+xZi4uXLigCxcuKDfXccInOjpakvI8iAMAAHfkbgs6nUoudu/erUGDBunQoUP2F5jZbDYZY2Sz2ZSTk2NpkAAAlGasuSiEZ599Vg0aNNCCBQsUFBQkm5vVkgAAQMGcSi5OnDih5cuXq169elbHAwBAmVMSj/8uSU4t6HzwwQe1f/9+q2MBAKBMypXNkqO0cGrmYv78+Ro0aJAOHjyopk2b5nnl+qOPPmpJcAAAlAUs6CyE5ORkbdu2TWvXrs1zjgWdAAC4N6fKIi+++KKeeuoppaWlKTc31+EgsQAAwFGuzZqjtHBq5uLSpUsaM2aMgoKCrI4HAIAyx922ojo1c9GvXz9t3rzZ6lgAAEAZ4NTMRYMGDZSQkKCtW7cqKioqz4LOkSNHWhIcAABlgbst6LSZnx+xWQQREREFD2iz6cSJE8UKqrg8vGqW6P2Be1XWuW9KOgTgnuNZrY7L77HgvqcsGWfImQ8sGcfVnJq5OHnypNVxAACAMoJXrgMA4GLutqDT6eTizJkzWr16tVJTU3Xz5k2Hc9OmTSt2YAAAlBUkF4WwadMmPfroo6pTp44OHz6spk2b6tSpUzLGqGXLllbHCAAAShGntqImJCRo3LhxOnDggHx8fLR8+XL9+9//VqdOndS/f3+rYwQAoFQzNmuO0sKp5OLQoUOKj4+XJHl4eCgrK0v+/v6aMmWK3njjDUsDBACgtMu16CgtnEouKlSoYF9nERISopSUFPu5//znP9ZEBgBAGeFuyYVTay7atm2rrVu3KjIyUg8//LBeeuklHThwQCtWrFDbtm2tjhEAAJQiTiUX06ZNU2ZmpiRp8uTJyszM1CeffKL69euzUwQAgN9wtyd0Fjm5yMnJ0ZkzZxQdHS3pfyWS2bNnWx4YAABlRWl6o6kVirzmonz58nrooYd0+fJlV8QDAABKOacWdDZt2rTE3x8CAEBp4W4LOp1KLl599VWNGzdOn3/+udLS0pSRkeFwAACAX7hbcuHUgs6HH35YkvToo4/KZvulkGSMkc1mU05OjjXRAQCAUsep5GLz5s1WxwEAQJnFbpFC6NSpk9VxAABQZrnbbpFivXL9+vXr+b4V9edtqgAAwP04lVxcvHhRzzzzjNauXZvvedZcAADwi9K0GNMKTu0WGT16tK5cuaLvvvtOvr6+Wrdund577z3Vr19fq1evtjpGAABKNWPRUVo4NXPx1Vdf6bPPPlOrVq1Urlw5hYWFqVu3bqpUqZISExPVs2dPq+MEAKDUyi1VqUHxOTVz8dNPP6lGjRqSpMDAQF28eFGSFBUVpT179lgXHQAAKHWcSi4aNmyoI0eOSJKaNWumOXPm6OzZs5o9e7ZCQkIsDRAAgNKOh2gVwqhRo5SWliZJmjhxorp3764PPvhAXl5eeu+99ywNEACA0s69iiJOJhdPPfWU/c8tW7bU6dOndfjwYdWuXVvVqlWzLDgAAFD6OFUWkaQFCxaoadOm8vHxUWBgoOLj47Vq1SoLQwMAoGygLFIIr7zyiqZNm6YXX3xRMTExkqTk5GSNGTNGqampmjJliqVBAgBQmvGEzkKYNWuW5s2bp7i4OHvbo48+qujoaL344oskFwAAuDGnkotbt26pVatWedrvv/9+/fe//y12UAAAlCU856IQnn76ac2aNStP+9y5czVw4MBiBwUAQFnCEzoLacGCBVq/fr3atm0rSfruu++Umpqq+Ph4jR071t5v2rRpxY8SAACUGk4lFwcPHlTLli0lSSkpKZKkatWqqVq1ajp48KC9n83mZitYAADIR2na6WEFp5KLzZs3Wx0HAABllrutuXC6LAIAAArHvVKLYjxECwAAID/MXAAA4GKsuQAAAJZytzUXlEUAAIClmLkAAMDF3GveguQCAACXc7c1F5RFAACApZi5AADAxYybFUZILgAAcDHKIgAAAMXAzAUAAC7mbs+5ILkAAMDF3Cu1ILkAAMDl3G3mgjUXAADAUsxcAADgYu62W4TkAgAAF3O351xQFgEAAJZi5gIAABejLAIAACxFWQQAAKAYmLkAAMDF3K0swswFAAAulmuMJUdRTJo0STabzeFo1KhRgf0XLVqUp7+Pj49T35eZCwAAyqgmTZpo48aN9s8eHrf/a79SpUo6cuSI/bPNZnPqviQXAAC4WEkt5/Tw8FBwcHCh+9tstiL1LwhlEQAAXCxXxpIjOztbGRkZDkd2dnaB9z127JhCQ0NVp04dDRw4UKmpqbeNMzMzU2FhYapVq5Z69+6t77//3qnvS3IBAICLGYv+SUxMVEBAgMORmJiY7z3btGmjRYsWad26dZo1a5ZOnjypjh076tq1a/n2b9iwod5991199tln+uCDD5Sbm6t27drpzJkzRf6+NmOKuEKkFPDwqlnSIQD3pKxz35R0CMA9x7NaHZffIy6sjyXjLDr6SZ6ZCm9vb3l7e9/x2itXrigsLEzTpk3TkCFD7tj/1q1bioyMVFxcnKZOnVqkOFlzAQCAi1m1FbWwiUR+KleurAYNGuj48eOF6u/p6akWLVoUuv+vURYBAMDFrFpzURyZmZlKSUlRSEhIofrn5OTowIEDhe7/ayQXAACUQePGjdPXX3+tU6dOafv27erbt6/Kly+vuLg4SVJ8fLwSEhLs/adMmaL169frxIkT2rNnj5566imdPn1aQ4cOLfK9KYsAAOBiJfFukTNnziguLk6XLl1S9erV1aFDB3377beqXr26JCk1NVXlyv0yx3D58mUNGzZM6enpCgwM1P3336/t27ercePGRb43CzoBN8KCTiCvu7Ggs1/Yo5aMs+L0akvGcTXKIgAAwFKURQAAcLEyWCS4LZILAABcrLg7PUobyiIAAMBSzFwAAOBiVj1Eq7QguQAAwMVKYitqSSK5AADAxVhzAQAAUAzMXAAA4GJsRQUAAJZytwWdlEUAAIClmLkAAMDF2C0CAAAsxW4RAACAYmDmAgAAF2O3CAAAsBRlEQAAgGJg5gIAABdjtwgAALBULmsuAACAldwrtWDNBQAAsBgzFwAAuJi77RYhuQAAwMXcLbmgLAIAACzFzAUAAC7GEzoBAIClKIsAAAAUAzMXAAC4GE/oBAAAlmLNBQAAsBRrLgAAAIqBmQsAAFyMsggAALAUZREAAIBiYOYCAAAXYysqAACwVK6brbmgLAIAACzFzAUAAC5GWQQAAFiKsggAAEAxMHMBAICLURYBAACWcreyCMkFAAAu5m4zF6y5AAAAlmLmAgAAF6MsAgAALEVZBAAAoBiYuQAAwMWMyS3pEO4qkgsAAFwsl7IIAACA85i5AADAxQy7RQAAgJUoiwAAABQDMxcAALgYZREAAGApntAJAAAsxRM6AQAAioGZCwAAXIw1FwAAwFJsRQUAACgGZi4AAHAxyiIAAMBS7rYVlbIIAACwFDMXAAC4GGURAABgKXaLAAAAFAMzFwAAuBhlEQAAYCl32y1CcgEAgIvx4jIAAIBiYOYCAAAXc7eyCDMXAAC4mDHGkqMoJk2aJJvN5nA0atTottcsW7ZMjRo1ko+Pj6KiorRmzRqnvi/JBQAAZVSTJk2UlpZmP7Zu3Vpg3+3btysuLk5DhgzR3r171adPH/Xp00cHDx4s8n1JLgAAcDFj0T9F5eHhoeDgYPtRrVq1Avv+9a9/Vffu3TV+/HhFRkZq6tSpatmypWbOnFnk+5JcAADgYlaVRbKzs5WRkeFwZGdnF3jfY8eOKTQ0VHXq1NHAgQOVmppaYN/k5GR17drVoS02NlbJyclF/r4kFwAAlBKJiYkKCAhwOBITE/Pt26ZNGy1atEjr1q3TrFmzdPLkSXXs2FHXrl3Lt396erqCgoIc2oKCgpSenl7kONktAgCAi1n1hM6EhASNHTvWoc3b2zvfvj169LD/OTo6Wm3atFFYWJiWLl2qIUOGWBJPQUguAABwMas2onp7exeYTNxJ5cqV1aBBAx0/fjzf88HBwTp//rxD2/nz5xUcHFzke1EWAQDADWRmZiolJUUhISH5no+JidGmTZsc2jZs2KCYmJgi36tMzlz89+bZkg4B/7/s7GwlJiYqISHB6WwbKGv4vXA/JfH30rhx49SrVy+FhYXp3LlzmjhxosqXL6+4uDhJUnx8vGrWrGlfszFq1Ch16tRJSUlJ6tmzpz7++GPt2rVLc+fOLfK9mbmAS2VnZ2vy5Mm3Xc0MuBt+L3A3nDlzRnFxcWrYsKEef/xxVa1aVd9++62qV68uSUpNTVVaWpq9f7t27fTRRx9p7ty5atasmT799FOtWrVKTZs2LfK9bcbd3gOLuyojI0MBAQG6evWqKlWqVNLhAPcEfi9Q1jFzAQAALEVyAQAALEVyAZfy9vbWxIkTWbQG/Aq/FyjrWHMBAAAsxcwFAACwFMkFAACwFMkFANwl4eHhmj59eqH7b9myRTabTVeuXHFZTIArsOYCTps0aZJWrVqlffv2lXQoQKlw8eJFVahQQX5+foXqf/PmTf34448KCgqSzWZzcXSAdcrk479xb7l165Y8PT1LOgygxP38ZMTC8vLycuqlUUBJoyzixnJzc5WYmKiIiAj5+vraH/cq/TIdu2nTJrVq1Up+fn5q166djhw5IklatGiRJk+erP3798tms8lms2nRokWSJJvNplmzZunRRx9VhQoV9Je//EWSNGvWLNWtW1deXl5q2LCh3n//fYd4fr6uR48e8vX1VZ06dezxSNIDDzygESNGOFxz8eJFeXl55XnZDlASrl27poEDB6pChQoKCQnR22+/rc6dO2v06NGS8pZFbDab5s+fr759+8rPz0/169fX6tWr7ecpi6DUMnBbr776qmnUqJFZt26dSUlJMQsXLjTe3t5my5YtZvPmzUaSadOmjdmyZYv5/vvvTceOHU27du2MMcZcv37dvPTSS6ZJkyYmLS3NpKWlmevXrxtjjJFkatSoYd59912TkpJiTp8+bVasWGE8PT3NO++8Y44cOWKSkpJM+fLlzVdffWWPR5KpWrWqmTdvnjly5Ih5+eWXTfny5c0PP/xgjDHmww8/NIGBgebGjRv2a6ZNm2bCw8NNbm7uXfzJAfkbOnSoCQsLMxs3bjQHDhwwffv2NRUrVjSjRo0yxhgTFhZm3n77bXt/Sea+++4zH330kTl27JgZOXKk8ff3N5cuXTLGGPvv4eXLl+/+lwGKgeTCTd24ccP4+fmZ7du3O7QPGTLExMXF2f9PbePGjfZzX3zxhZFksrKyjDHGTJw40TRr1izP2JLM6NGjHdratWtnhg0b5tDWv39/8/DDDztcN3z4cIc+bdq0Mc8995wxxpisrCwTGBhoPvnkE/v56OhoM2nSpCJ8c8A1MjIyjKenp1m2bJm97cqVK8bPz++2ycXLL79s/5yZmWkkmbVr1xpjSC5QelEWcVPHjx/X9evX1a1bN/n7+9uPxYsXKyUlxd4vOjra/ueQkBBJ0oULF+44fqtWrRw+Hzp0SO3bt3doa9++vQ4dOuTQFhMTk+fzz318fHz09NNP691335Uk7dmzRwcPHtTgwYPvGA/gaidOnNCtW7fUunVre1tAQIAaNmx42+t+/TtWoUIFVapUqVC/Y8C9jAWdbiozM1OS9MUXX6hmzZoO57y9ve0Jxq8XYv68Wj03N/eO41eoUMGqUB0MHTpUzZs315kzZ7Rw4UI98MADCgsLc8m9gLvht4udbTZboX7HgHsZMxduqnHjxvL29lZqaqrq1avncNSqVatQY3h5eSknJ6dQfSMjI7Vt2zaHtm3btqlx48YObd9++22ez5GRkfbPUVFRatWqlebNm6ePPvpIzz77bKHuD7hanTp15OnpqZ07d9rbrl69qqNHj5ZgVEDJYObCTVWsWFHjxo3TmDFjlJubqw4dOujq1avatm2bKlWqVKjZgPDwcJ08eVL79u3Tfffdp4oVKxb4Iqbx48fr8ccfV4sWLdS1a1f94x//0IoVK7Rx40aHfsuWLVOrVq3UoUMHffjhh9qxY4cWLFjg0Gfo0KEaMWKEKlSooL59+zr/QwAsVLFiRQ0aNEjjx49XlSpVVKNGDU2cOFHlypXjGRVwO8xcuLGpU6dqwoQJSkxMVGRkpLp3764vvvhCERERhbr+97//vbp3764uXbqoevXqWrJkSYF9+/Tpo7/+9a9666231KRJE82ZM0cLFy5U586dHfpNnjxZH3/8saKjo7V48WItWbIkz+xGXFycPDw8FBcXJx8fnyJ/b8BVpk2bppiYGD3yyCPq2rWr2rdvr8jISP53CrfDEzpxz7DZbFq5cqX69Olz236nTp1S3bp1tXPnTrVs2fLuBAc44aefflLNmjWVlJSkIUOGlHQ4wF1DWQSlxq1bt3Tp0iW9/PLLatu2LYkF7jl79+7V4cOH1bp1a129elVTpkyRJPXu3buEIwPuLpILlBrbtm1Tly5d1KBBA4cndwL3krfeektHjhyRl5eX7r//fn3zzTeqVq1aSYcF3FWURQAAgKVY0AkAACxFcgEAACxFcgEAACxFcgEAACxFcgEAACxFcgEgXzabTatWrXLJ2IsWLVLlypVdMjaAksdWVAD5Sk9PV2BgoLy9vXXq1ClFRERo7969at68ebHHzsrK0rVr11SjRo3iBwrgnsNDtIC7wBijnJwceXjc+79yN2/elJeXl4KDg10y/q1bt+Tr6ytfX1+XjA+g5FEWAfLRuXNnjRgxQiNGjFBAQICqVaumCRMm6OeJvvfff1+tWrVSxYoVFRwcrCeffFIXLlywX79lyxbZbDatXbtW999/v7y9vbV161alpKSod+/eCgoKkr+/v373u9/leTNseHi4Xn31VcXHx8vf319hYWFavXq1Ll68qN69e8vf31/R0dHatWtXob/Ptm3b1LlzZ/n5+SkwMFCxsbG6fPmyw3cdPXq0qlWrptjYWEmOZZGfX2bXokUL2Ww2hxfOzZ8/3/5yrkaNGunvf/+7/dypU6dks9n0ySefqFOnTvLx8dGHH36Yb1lk1qxZqlu3rry8vNSwYUO9//77DudtNpvmz5+vvn37ys/PT/Xr19fq1asL/TMAcBcZAHl06tTJ+Pv7m1GjRpnDhw+bDz74wPj5+Zm5c+caY4xZsGCBWbNmjUlJSTHJyckmJibG9OjRw3795s2bjSQTHR1t1q9fb44fP24uXbpk9u3bZ2bPnm0OHDhgjh49al5++WXj4+NjTp8+bb82LCzMVKlSxcyePdscPXrUPPfcc6ZSpUqme/fuZunSpebIkSOmT58+JjIy0uTm5t7xu+zdu9d4e3ub5557zuzbt88cPHjQzJgxw1y8eNHhu44fP94cPnzYHD582BhjjCSzcuVKY4wxO3bsMJLMxo0bTVpamrl06ZIxxpgPPvjAhISEmOXLl5sTJ06Y5cuXmypVqphFixYZY4w5efKkkWTCw8Ptfc6dO2cWLlxoAgIC7DGuWLHCeHp6mnfeecccOXLEJCUlmfLly5uvvvrK3keSue+++8xHH31kjh07ZkaOHGn8/f3tsQC4d5BcAPno1KlTnr+8//SnP5nIyMh8++/cudNIMteuXTPG/JJcrFq16o73atKkiZkxY4b9c1hYmHnqqafsn9PS0owkM2HCBHtbcnKykWTS0tLuOH5cXJxp3759gec7depkWrRokaf918nFz0nC3r17HfrUrVvXfPTRRw5tU6dONTExMQ7XTZ8+3aHPb5OLdu3amWHDhjn06d+/v3n44Ycd4nn55ZftnzMzM40ks3bt2gK/G4CSQVkEKEDbtm1ls9nsn2NiYnTs2DHl5ORo9+7d6tWrl2rXrq2KFSuqU6dOkqTU1FSHMVq1auXwOTMzU+PGjVNkZKQqV64sf39/HTp0KM910dHR9j8HBQVJkqKiovK0/boUU5B9+/bpwQcfvG2f+++//47j/NZPP/2klJQUDRkyRP7+/vbj1VdfVUpKikPf3/4cfuvQoUNq3769Q1v79u116NAhh7Zf/1wqVKigSpUqFepnAODuuvdXlwH3mBs3big2NlaxsbH68MMPVb16daWmpio2NlY3b9506FuhQgWHz+PGjdOGDRv01ltvqV69evL19dVjjz2W5zpPT0/7n39OcPJry83NvWO8hVk4+ds4CyMzM1OSNG/ePLVp08bhXPny5Ys9fn5+/TOQ/vdzKMzPAMDdxcwFUIDvvvvO4fO3336r+vXr6/Dhw7p06ZJef/11dezYUY0aNSr0fz1v27ZNgwcPVt++fRUVFaXg4GCdOnXKBdH/Ijo6Wps2bSrWGF5eXpKknJwce1tQUJBCQ0N14sQJ1atXz+H4eQFoYUVGRmrbtm0Obdu2bVPjxo2LFTeAksHMBVCA1NRUjR07Vn/84x+1Z88ezZgxQ0lJSapdu7a8vLw0Y8YMDR8+XAcPHtTUqVMLNWb9+vW1YsUK9erVSzabTRMmTHD5f3knJCQoKipKzz//vIYPHy4vLy9t3rxZ/fv3V7Vq1Qo1Ro0aNeTr66t169bpvvvuk4+PjwICAjR58mSNHDlSAQEB6t69u7Kzs7Vr1y5dvnxZY8eOLXSM48eP1+OPP64WLVqoa9eu+sc//qEVK1bk2UkDoHRg5gIoQHx8vLKystS6dWu98MILGjVqlP7whz+oevXqWrRokZYtW6bGjRvr9ddf11tvvVWoMadNm6bAwEC1a9dOvXr1UmxsrFq2bOnS79GgQQOtX79e+/fvV+vWrRUTE6PPPvusSM/c8PDw0N/+9jfNmTNHoaGh6t27tyRp6NChmj9/vhYuXKioqCh16tRJixYtKvLMRZ8+ffTXv/5Vb731lpo0aaI5c+Zo4cKFDlteAZQePKETyEfnzp3VvHlzTZ8+vaRDAYBSh5kLAABgKZILoJTr0aOHw1bQXx+vvfZaSYcHwA1RFgFKubNnzyorKyvfc1WqVFGVKlXuckQA3B3JBQAAsBRlEQAAYCmSCwAAYCmSCwAAYCmSCwAAYCmSCwAAYCmSCwAAYCmSCwAAYCmSCwAAYKn/D0cGFCjZa5wTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "table = pd.pivot_table(pd.DataFrame(model.cv_results_),\n",
    "    values='mean_test_score', index='param_n_estimators', \n",
    "                       columns='param_criterion')\n",
    "     \n",
    "sns.heatmap(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate how our model performed using Random Search. In this case, using Random Search leads to a consistent increase in accuracy compared to our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[85280    19]\n",
      " [   45    99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85299\n",
      "           1       0.84      0.69      0.76       144\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.92      0.84      0.88     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionforest = model.best_estimator_.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionforest))\n",
    "print(classification_report(Y_Test,predictionforest))\n",
    "acc3 = accuracy_score(Y_Test,predictionforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Grid Search <a id=\"4\"></a> <br>\n",
    "In Grid Search, we set up a grid of hyperparameters and train/test our model on each of the possible combinations.\n",
    "In order to choose the parameters to use in Grid Search, we can now look at which parameters worked best with Random Search and form a grid based on them to see if we can find a better combination.\n",
    "\n",
    "Grid Search can be implemented in Python using scikit-learn GridSearchCV() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 36 candidates, totalling 144 fits\n",
      "[[85284    15]\n",
      " [   65    79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85299\n",
      "           1       0.84      0.55      0.66       144\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.92      0.77      0.83     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = {'criterion': ['entropy', 'gini'],\n",
    "               'max_depth': [2],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'min_samples_leaf': [4, 6, 8],\n",
    "               'min_samples_split': [5, 7,10],\n",
    "               'n_estimators': [20]}\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "model = GridSearchCV(estimator = clf, param_grid = grid_search, \n",
    "                               cv = 4, verbose= 5, n_jobs = -1)\n",
    "model.fit(X_Train,Y_Train)\n",
    "\n",
    "predictionforest = model.best_estimator_.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionforest))\n",
    "print(classification_report(Y_Test,predictionforest))\n",
    "acc4 = accuracy_score(Y_Test,predictionforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search is slower compared to Random Search but it can be overall more effective because it can go through the whole search space. Instead, Random Search can be faster fast but might miss some important points in the search space.\n",
    "# 5. Automated Hyperparameter Tuning <a id=\"5\"></a> <br>\n",
    "\n",
    "![](https://better.future-processing.com/directus/storage/uploads/2399317284eda5016daac68812d5d3c3.png)\n",
    "As we have seen above tuning machine learning hyperparameters is indeed a tedious but crucial task, as the performance of an algorithm can be highly dependent on the choice of hyperparameters. Manual tuning takes time away from important steps of the machine learning pipeline like feature engineering and interpreting results. Grid and random search are hands-off, but require long run times because they waste time evaluating unpromising areas of the search space. Increasingly, hyperparameter tuning is done by automated methods that aim to find optimal hyperparameters in less time using an informed search with no manual effort necessary beyond the initial set-up.\n",
    "\n",
    "When using Automated Hyperparameter Tuning, the model hyperparameters to use are identified using techniques such as: Bayesian Optimization, Gradient Descent and Evolutionary Algorithms.\n",
    "\n",
    "## Bayesian Optimization using HyperOpt <a id=\"51\"></a> <br>\n",
    "\n",
    "![](https://i.imgur.com/BWbgCSx.jpg)\n",
    "Bayesian optimization, a model-based method for finding the minimum of a function,while the final aim is to find the input value to a function which can give us the lowest possible output value has resulted in achieving better performance while requiring fewer iterations than random search.  Bayesian Optimization can, therefore, lead to better performance in the testing phase and reduced optimization time.\n",
    "\n",
    "Bayesian Optimization can be performed in Python using the Hyperopt library.  \n",
    "\n",
    "![](https://camo.githubusercontent.com/b92ead141ef3726da38eef053864aa1173012789/68747470733a2f2f692e706f7374696d672e63632f54506d66665772702f68797065726f70742d6e65772e706e67)\n",
    "\n",
    "In Hyperopt, Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin().\n",
    "\n",
    "* **Objective Function** = defines the loss function to minimize.\n",
    "* **Domain Space** = defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).\n",
    "* **Optimization Algorithm** = defines the search algorithm to use to select the best input values to use in each new iteration.\n",
    "\n",
    "Additionally, can also be defined in **fmin()** the maximum number of evaluations to perform.\n",
    "\n",
    "Bayesian Optimization can reduce the number of search iterations by choosing the input values bearing in mind the past outcomes. In this way, we can concentrate our search from the beginning on values which are closer to our desired output.\n",
    "We can now run our Bayesian Optimizer using the fmin() function. A Trials() object is first created to make possible to visualize later what was going on while the **fmin()** function was running (eg. how the loss function was changing and how to used Hyperparameters were changing).\n",
    "\n",
    "\n",
    "Hyperopt is one of several automated hyperparameter tuning libraries using Bayesian optimization. These libraries differ in the algorithm used to both construct the surrogate (probability model) of the objective function and choose the next hyperparameters to evaluate in the objective function. Hyperopt uses the Tree Parzen Estimator (TPE). Other Python libraries include Spearmint, which uses a Gaussian process for the surrogate, and SMAC, which uses a random forest regression.\n",
    "\n",
    "Hyperopt has a simple syntax for structuring an optimization problem which extends beyond hyperparameter tuning to any problem that involves minimizing a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from hyperopt) (1.23.5)\n",
      "Requirement already satisfied: six in c:\\users\\iyers\\appdata\\roaming\\python\\python39\\site-packages (from hyperopt) (1.16.0)\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "     ------------------------------------- 200.5/200.5 kB 11.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from hyperopt) (2.8.4)\n",
      "Requirement already satisfied: future in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from hyperopt) (0.18.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from hyperopt) (4.64.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from hyperopt) (2.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from hyperopt) (1.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\iyers\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
      "Installing collected packages: py4j, hyperopt\n",
      "Successfully installed hyperopt-0.2.7 py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                           | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: \n",
      "All the 4 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got 10.0 instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 0/20 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 4 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n    self._validate_params()\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got 10.0 instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39maccuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK }\n\u001b[0;32m     25\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[1;32m---> 26\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m best\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(space)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(space):\n\u001b[0;32m     12\u001b[0m     model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(criterion \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcriterion\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     13\u001b[0m                                    max_depth \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     14\u001b[0m                                  max_features \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_features\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m                                  n_estimators \u001b[38;5;241m=\u001b[39m space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     18\u001b[0m                                  )\n\u001b[1;32m---> 20\u001b[0m     accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_Train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_Train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# We aim to maximize accuracy, therefore we return it as a negative value\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39maccuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m: STATUS_OK }\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:285\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m    266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[1;32m--> 285\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(scoring):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 4 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 340, in fit\n    self._validate_params()\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 600, in _validate_params\n    validate_parameter_constraints(\n  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'max_depth' parameter of RandomForestClassifier must be an int in the range [1, inf) or None. Got 10.0 instead.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
    "        'max_depth': hp.quniform('max_depth', 10, 12, 10),\n",
    "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n",
    "        'min_samples_leaf': hp.uniform ('min_samples_leaf', 0, 0.5),\n",
    "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
    "        'n_estimators' : hp.choice('n_estimators', [10, 50])\n",
    "    }\n",
    "\n",
    "def objective(space):\n",
    "    model = RandomForestClassifier(criterion = space['criterion'], \n",
    "                                   max_depth = space['max_depth'],\n",
    "                                 max_features = space['max_features'],\n",
    "                                 min_samples_leaf = space['min_samples_leaf'],\n",
    "                                 min_samples_split = space['min_samples_split'],\n",
    "                                 n_estimators = space['n_estimators'], \n",
    "                                 )\n",
    "    \n",
    "    accuracy = cross_val_score(model, X_Train, Y_Train, cv = 4).mean()\n",
    "\n",
    "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK }\n",
    "    \n",
    "trials = Trials()\n",
    "best = fmin(fn= objective,\n",
    "            space= space,\n",
    "            algo= tpe.suggest,\n",
    "            max_evals = 20,\n",
    "            trials= trials)\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now retrieve the set of **best** parameters identified and test our model using the **best** dictionary created during training. Some of the parameters have been stored in the **best** dictionary numerically using indices, therefore, we need first to convert them back as strings before input them in our Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = {0: 'entropy', 1: 'gini'}\n",
    "feat = {0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\n",
    "est = {0: 10, 1: 50, 2: 75, 3: 100, 4: 125}\n",
    "\n",
    "trainedforest = RandomForestClassifier(criterion = crit[best['criterion']], \n",
    "                                       max_depth = best['max_depth'], \n",
    "                                       max_features = feat[best['max_features']], \n",
    "                                       min_samples_leaf = best['min_samples_leaf'], \n",
    "                                       min_samples_split = best['min_samples_split'], \n",
    "                                       n_estimators = est[best['n_estimators']]\n",
    "                                      ).fit(X_Train,Y_Train)\n",
    "predictionforest = trainedforest.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,predictionforest))\n",
    "print(classification_report(Y_Test,predictionforest))\n",
    "acc5 = accuracy_score(Y_Test,predictionforest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic Algorithms using TPOT <a id=\"52\"></a> <br>\n",
    "In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection.\n",
    "\n",
    "Genetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts. They are inspired by the Darwinian process of Natural Selection and they are therefore also usually called as Evolutionary Algorithms.\n",
    "![](https://mctrans.ce.ufl.edu/featured/TRANSYT-7F/release9/genetic2.gif)\n",
    "Letâ€™s imagine we create a population of N Machine Learning models with some predefined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that perform best). We can now generate some **offsprings** having similar Hyperparameters to the ones of the best models so that to get again a population of N models. At this point, we can again calculate the accuracy of each model and repeat the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.\n",
    "\n",
    "In order to implement Genetic Algorithms in Python, we can use the TPOT Auto Machine Learning library. TPOT is built on the scikit-learn library and it can be used for either regression or classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "pip install deap update_checker tqdm stopit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "parameters = {'criterion': ['entropy', 'gini'],\n",
    "               'max_depth': [2],\n",
    "               'max_features': ['auto'],\n",
    "               'min_samples_leaf': [4, 12],\n",
    "               'min_samples_split': [5, 10],\n",
    "               'n_estimators': [10]}\n",
    "               \n",
    "tpot_classifier = TPOTClassifier(generations= 4, population_size= 24, offspring_size= 12,\n",
    "                                 verbosity= 2, early_stop= 12,\n",
    "                                 config_dict=\n",
    "                                 {'sklearn.ensemble.RandomForestClassifier': parameters}, \n",
    "                                 cv = 4, scoring = 'accuracy')\n",
    "tpot_classifier.fit(X_Train,Y_Train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training report and the best parameters are identified above using Genetic Algorithms. \n",
    "\n",
    "The overall accuracy of our Random Forest Genetic Algorithm optimized model is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tpot_classifier.score(X_Test, Y_Test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks (ANNs) Tuning <a id=\"53\"></a> <br>\n",
    "![](https://miro.medium.com/max/6000/1*wT6pIMnjZ9oArkidnVsGtg.png)\n",
    "Using KerasClassifier wrapper, it is possible to apply Grid Search and Random Search for Deep Learning models in the same way it was done when using scikit-learn Machine Learning models. In the following example, we will try to optimize some of our ANN parameters such as: how many neurons to use in each layer and which activation function and optimizer to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DL_Model(activation= 'linear', neurons= 5, optimizer='Adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim= 4, activation= activation))\n",
    "    model.add(Dense(neurons, activation= activation))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining grid parameters\n",
    "activation = ['softmax', 'relu']\n",
    "neurons = [5, 10]\n",
    "optimizer = ['Adam', 'Adamax']\n",
    "param_grid = dict(activation = activation, neurons = neurons, optimizer = optimizer)\n",
    "\n",
    "clf = KerasClassifier(build_fn= DL_Model, epochs= 5, batch_size=1024, verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GridSearchCV(estimator= clf, param_grid=param_grid, n_jobs=-1)\n",
    "model.fit(X_Train,Y_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max Accuracy Registred: {} using {}\".format(round(model.best_score_,3), \n",
    "                                                   model.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall accuracy scored using our Artificial Neural Network (ANN) can be viewed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_test = model.predict(X_Test)\n",
    "print(confusion_matrix(Y_Test,prediction_test))\n",
    "print(classification_report(Y_Test,prediction_test))\n",
    "accuracy_ANN = accuracy_score(Y_Test,prediction_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Optuna <a id=\"6\"></a> <br>\n",
    "![](https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png)\n",
    "\n",
    "Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Optuna is a framework designed for the automation and the acceleration of the optimization studies.\n",
    "\n",
    "**Key Features:**\n",
    "\n",
    "* **Eager search spaces**: Automated search for optimal hyperparameters using Python conditionals, loops, and syntax\n",
    "\n",
    "* **State-of-the-art algorithms**: Efficiently search large spaces and prune unpromising trials for faster results\n",
    "\n",
    "* **Easy parallelization**: Parallelize hyperparameter searches over multiple threads or processes without modifying code\n",
    "\n",
    "We use the terms **study** and **trial** as follows:\n",
    "\n",
    "**Study**: optimization based on an objective function\n",
    "\n",
    "**Trial**: a single execution of the objective function\n",
    "\n",
    "The goal of a study is to find out the optimal set of hyperparameter values (e.g., classifier and svm_c) through multiple trials (e.g., n_trials=100). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.1.1-py3-none-any.whl (365 kB)\n",
      "     -------------------------------------- 365.7/365.7 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from optuna) (23.1)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
      "     ---------------------------------------- 224.5/224.5 kB ? eta 0:00:00\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.7/78.7 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\iyers\\appdata\\roaming\\python\\python39\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
      "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
      "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimize Scikit-Learn hyperparameters, such as the C parameter of SVC and the max_depth of the RandomForestClassifier, in three steps:\n",
    "\n",
    "* Wrap model training with an objective function and return accuracy\n",
    "* Suggest hyperparameters using a trial object\n",
    "* Create a study object and execute the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-27 23:37:54,919]\u001b[0m A new study created in memory with name: no-name-3e58206f-6364-40d3-83c7-d35c450aeb59\u001b[0m\n",
      "C:\\Users\\iyers\\AppData\\Local\\Temp\\ipykernel_16132\\567779602.py:18: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n",
      "\u001b[33m[W 2023-05-27 23:37:54,935]\u001b[0m Trial 0 failed with parameters: {'classifier': 'RandomForest', 'rf_max_depth': 13.443731877245726} because of the following error: NameError(\"name 'accuracy' is not defined\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\iyers\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\iyers\\AppData\\Local\\Temp\\ipykernel_16132\\567779602.py\", line 21, in objective\n",
      "    return accuracy\n",
      "NameError: name 'accuracy' is not defined\n",
      "\u001b[33m[W 2023-05-27 23:37:54,943]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 3. Create a study object and optimize the objective function.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     19\u001b[0m     classifier_obj \u001b[38;5;241m=\u001b[39m sklearn\u001b[38;5;241m.\u001b[39mensemble\u001b[38;5;241m.\u001b[39mRandomForestClassifier(max_depth\u001b[38;5;241m=\u001b[39mrf_max_depth, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maccuracy\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "import optuna\n",
    "\n",
    "# 1. Define an objective function to be maximized.\n",
    "def objective(trial):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    x, y = iris.data, iris.target\n",
    "    # 2. Suggest values for the hyperparameters using a trial object.\n",
    "    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest'])\n",
    "    if classifier_name == 'SVC':\n",
    "         svc_c = trial.suggest_loguniform('svc_c', 1e-10, 1e10)\n",
    "         classifier_obj = sklearn.svm.SVC(C=svc_c, gamma='auto')\n",
    "    else:\n",
    "        rf_max_depth = int(trial.suggest_loguniform('rf_max_depth', 2, 32))\n",
    "        classifier_obj = sklearn.ensemble.RandomForestClassifier(max_depth=rf_max_depth, n_estimators=10)\n",
    "    ...\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Tune <a id=\"7\"></a> <br>\n",
    "![](https://miro.medium.com/max/3622/1*GsJLYcS5W2tCfHg4NDOscA.png)\n",
    "Tune is a Python library for experiment execution and hyperparameter tuning at any scale. Core features:\n",
    "\n",
    "* Launch a multi-node distributed hyperparameter sweep in less than 10 lines of code.\n",
    "\n",
    "* Supports any machine learning framework, including PyTorch, XGBoost, MXNet, and Keras.\n",
    "\n",
    "* Natively integrates with optimization libraries such as HyperOpt, Bayesian Optimization, and Facebook Ax.\n",
    "\n",
    "* Choose among scalable algorithms such as Population Based Training (PBT), Vizierâ€™s Median Stopping Rule, HyperBand/ASHA.\n",
    "\n",
    "* Visualize results with TensorBoard.\n",
    "\n",
    "* Move your models from training to serving on the same infrastructure with Ray Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "!pip install 'ray[tune]' torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from ray import tune\n",
    "from ray.tune.examples.mnist_pytorch import get_data_loaders, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(config):\n",
    "    train_loader, test_loader = get_data_loaders()\n",
    "    model = ConvNet()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "    for i in range(10):\n",
    "        train(model, optimizer, train_loader)\n",
    "        acc = test(model, test_loader)\n",
    "        tune.report(mean_accuracy=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = tune.run(train_mnist, config={\"lr\": tune.grid_search([0.001, 0.01, 0.1])})\n",
    "print(\"Best config: \", analysis.get_best_config(metric=\"mean_accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataframe for analyzing trial results.\n",
    "df = analysis.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sherpa <a id=\"8\"></a> <br>\n",
    "![](https://camo.githubusercontent.com/3e051525488a679b1489251621ab906bb66b597d/68747470733a2f2f646f63732e676f6f676c652e636f6d2f64726177696e67732f642f652f32504143582d317652615450356435577154344b59345635376e6949347746446b7a303039387a4854527a5a396e37537a7a4674644e35616b42643735486368426e6859492d4750765f415948317a5961304f325f302f7075623f773d35323226683d313530)\n",
    "\n",
    "Sherpa can automatically run parallel evaluations on a cluster using a job scheduler such as SGE. Simply provide a Python script that takes a set of hyperparameters as arguments and performs a single trial evaluation. A database collects the partial results in real-time, and the hyperparameter optimization algorithm decides what to do next.\n",
    "\n",
    "SHERPA is a Python library for hyperparameter tuning of machine learning models. It provides:\n",
    "\n",
    "* Hyperparameter optimization for machine learning researchers\n",
    "* It can be used with any Python machine learning library such as Keras, Tensorflow, PyTorch, or Scikit-Learn\n",
    "* A choice of hyperparameter optimization algorithms such as Bayesian optimization via GPyOpt, Asynchronous Successive Halving (aka Hyperband) , and Population Based Training .\n",
    "* Parallel computation that can be fitted to the user's needs\n",
    "* A live dashboard for the exploratory analysis of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting parameter-sherpa\n",
      "  Downloading parameter-sherpa-1.0.6.tar.gz (513 kB)\n",
      "     -------------------------------------- 513.5/513.5 kB 4.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=0.20.3 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from parameter-sherpa) (1.5.3)\n",
      "Collecting pymongo>=3.5.1\n",
      "  Downloading pymongo-4.3.3-cp39-cp39-win_amd64.whl (382 kB)\n",
      "     ------------------------------------- 382.5/382.5 kB 11.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from parameter-sherpa) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from parameter-sherpa) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from parameter-sherpa) (1.2.2)\n",
      "Requirement already satisfied: flask>=0.12.2 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from parameter-sherpa) (1.1.2)\n",
      "Collecting GPyOpt>=1.2.5\n",
      "  Downloading GPyOpt-1.2.6.tar.gz (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting enum34\n",
      "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from parameter-sherpa) (3.6.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from flask>=0.12.2->parameter-sherpa) (2.0.3)\n",
      "Requirement already satisfied: click>=5.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from flask>=0.12.2->parameter-sherpa) (8.0.4)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from flask>=0.12.2->parameter-sherpa) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from flask>=0.12.2->parameter-sherpa) (2.0.1)\n",
      "Collecting GPy>=1.8\n",
      "  Downloading GPy-1.12.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 18.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from pandas>=0.20.3->parameter-sherpa) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\iyers\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=0.20.3->parameter-sherpa) (2.8.2)\n",
      "Collecting dnspython<3.0.0,>=1.16.0\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "     ------------------------------------- 283.7/283.7 kB 18.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->parameter-sherpa) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->parameter-sherpa) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from matplotlib->parameter-sherpa) (1.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from matplotlib->parameter-sherpa) (1.4.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from matplotlib->parameter-sherpa) (9.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from matplotlib->parameter-sherpa) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from matplotlib->parameter-sherpa) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from matplotlib->parameter-sherpa) (23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from matplotlib->parameter-sherpa) (4.25.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\iyers\\appdata\\roaming\\python\\python39\\site-packages (from click>=5.1->flask>=0.12.2->parameter-sherpa) (0.4.6)\n",
      "Requirement already satisfied: cython>=0.29 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (0.29.32)\n",
      "Requirement already satisfied: six in c:\\users\\iyers\\appdata\\roaming\\python\\python39\\site-packages (from GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (1.16.0)\n",
      "Collecting paramz>=0.9.0\n",
      "  Downloading paramz-0.9.5.tar.gz (71 kB)\n",
      "     ---------------------------------------- 71.3/71.3 kB 3.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\iyers\\anaconda3\\lib\\site-packages (from Jinja2>=2.10.1->flask>=0.12.2->parameter-sherpa) (2.0.1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\iyers\\appdata\\roaming\\python\\python39\\site-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt>=1.2.5->parameter-sherpa) (5.1.1)\n",
      "Building wheels for collected packages: parameter-sherpa, GPyOpt, paramz\n",
      "  Building wheel for parameter-sherpa (setup.py): started\n",
      "  Building wheel for parameter-sherpa (setup.py): finished with status 'done'\n",
      "  Created wheel for parameter-sherpa: filename=parameter_sherpa-1.0.6-py2.py3-none-any.whl size=542118 sha256=17ccd46186b998029d49e7c4586ffb795e640a9764504d9f64f0fb2e1feced2a\n",
      "  Stored in directory: c:\\users\\iyers\\appdata\\local\\pip\\cache\\wheels\\5e\\e2\\ac\\eb3e175761289cb9a9c5d86045009b9e192ed0df15554f637d\n",
      "  Building wheel for GPyOpt (setup.py): started\n",
      "  Building wheel for GPyOpt (setup.py): finished with status 'done'\n",
      "  Created wheel for GPyOpt: filename=GPyOpt-1.2.6-py3-none-any.whl size=83599 sha256=e0f9c7d5d86e247eaf43e4aa4c31abd9e1afd8cd4d84b92dc751912a2baf9bba\n",
      "  Stored in directory: c:\\users\\iyers\\appdata\\local\\pip\\cache\\wheels\\11\\b8\\44\\0282cdff2277bc12f04266de6f104099dec02411879a0ac19f\n",
      "  Building wheel for paramz (setup.py): started\n",
      "  Building wheel for paramz (setup.py): finished with status 'done'\n",
      "  Created wheel for paramz: filename=paramz-0.9.5-py3-none-any.whl size=102549 sha256=27401acb4fa386cf557f827a5966189f2eba62e9d1b1e58e6a9a9e8f25c312ac\n",
      "  Stored in directory: c:\\users\\iyers\\appdata\\local\\pip\\cache\\wheels\\9c\\5f\\9b\\c4273ae8f869387214be2b99598d1b71dbf00672576cb85e74\n",
      "Successfully built parameter-sherpa GPyOpt paramz\n",
      "Installing collected packages: enum34, dnspython, pymongo, paramz, GPy, GPyOpt, parameter-sherpa\n",
      "Successfully installed GPy-1.12.0 GPyOpt-1.2.6 dnspython-2.3.0 enum34-1.1.10 parameter-sherpa-1.0.6 paramz-0.9.5 pymongo-4.3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install parameter-sherpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "import sherpa\n",
    "import sherpa.algorithms.bayesian_optimization as bayesian_optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [sherpa.Discrete('n_estimators', [2, 50]),\n",
    "              sherpa.Choice('criterion', ['gini', 'entropy']),\n",
    "              sherpa.Continuous('max_features', [0.1, 0.9])]\n",
    "\n",
    "algorithm = bayesian_optimization.GPyOpt(max_concurrent=1,model_type='GP_MCMC',acquisition_type='EI_MCMC',max_num_trials=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial  1  with parameters  {'n_estimators': 3, 'criterion': 'gini', 'max_features': 0.6832416174867602}\n",
      "Score:  0.9402421984163949\n",
      "Trial  2  with parameters  {'n_estimators': 47, 'criterion': 'entropy', 'max_features': 0.7840801431867024}\n",
      "Score:  0.9543083372147182\n",
      "Trial  3  with parameters  {'n_estimators': 25, 'criterion': 'entropy', 'max_features': 0.3882648760923879}\n",
      "Score:  0.9560937742586555\n",
      "Trial  4  with parameters  {'n_estimators': 16, 'criterion': 'gini', 'max_features': 0.6249591753707782}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GP:initializing Y\n",
      "INFO:GP:initializing inference method\n",
      "INFO:GP:adding kernel and likelihood as parameters\n",
      "WARNING:rbf:reconstraining parameters GP_regression.rbf\n",
      "WARNING:variance:reconstraining parameters GP_regression.Gaussian_noise.variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9490141282409563\n",
      "Trial  5  with parameters  {'n_estimators': 25, 'criterion': 'entropy', 'max_features': 0.3079027366995063}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GP:initializing Y\n",
      "INFO:GP:initializing inference method\n",
      "INFO:GP:adding kernel and likelihood as parameters\n",
      "WARNING:rbf:reconstraining parameters GP_regression.rbf\n",
      "WARNING:variance:reconstraining parameters GP_regression.Gaussian_noise.variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9648812296227295\n",
      "Trial  6  with parameters  {'n_estimators': 25, 'criterion': 'entropy', 'max_features': 0.28417329221232945}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GP:initializing Y\n",
      "INFO:GP:initializing inference method\n",
      "INFO:GP:adding kernel and likelihood as parameters\n",
      "WARNING:rbf:reconstraining parameters GP_regression.rbf\n",
      "WARNING:variance:reconstraining parameters GP_regression.Gaussian_noise.variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9595870206489675\n",
      "Trial  7  with parameters  {'n_estimators': 25, 'criterion': 'entropy', 'max_features': 0.30733073139205863}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GP:initializing Y\n",
      "INFO:GP:initializing inference method\n",
      "INFO:GP:adding kernel and likelihood as parameters\n",
      "WARNING:rbf:reconstraining parameters GP_regression.rbf\n",
      "WARNING:variance:reconstraining parameters GP_regression.Gaussian_noise.variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9648812296227295\n",
      "Trial  8  with parameters  {'n_estimators': 26, 'criterion': 'entropy', 'max_features': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GP:initializing Y\n",
      "INFO:GP:initializing inference method\n",
      "INFO:GP:adding kernel and likelihood as parameters\n",
      "WARNING:rbf:reconstraining parameters GP_regression.rbf\n",
      "WARNING:variance:reconstraining parameters GP_regression.Gaussian_noise.variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9648501785437043\n",
      "Trial  9  with parameters  {'n_estimators': 26, 'criterion': 'entropy', 'max_features': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GP:initializing Y\n",
      "INFO:GP:initializing inference method\n",
      "INFO:GP:adding kernel and likelihood as parameters\n",
      "WARNING:rbf:reconstraining parameters GP_regression.rbf\n",
      "WARNING:variance:reconstraining parameters GP_regression.Gaussian_noise.variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9648501785437043\n"
     ]
    }
   ],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "study = sherpa.Study(parameters=parameters,\n",
    "                     algorithm=algorithm,\n",
    "                     lower_is_better=False,\n",
    "                     disable_dashboard=True)\n",
    "\n",
    "for trial in study:\n",
    "    print(\"Trial \", trial.id, \" with parameters \", trial.parameters)\n",
    "    clf = RandomForestClassifier(criterion=trial.parameters['criterion'],\n",
    "                                 max_features=trial.parameters['max_features'],\n",
    "                                 n_estimators=trial.parameters['n_estimators'],\n",
    "                                 random_state=0)\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"Score: \", scores.mean())\n",
    "    study.add_observation(trial, iteration=1, objective=scores.mean())\n",
    "    study.finalize(trial)\n",
    "print(study.get_best_result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4a7247b-6654-483b-b8d5-a754c7834efe",
    "_uuid": "78e8638406e22164d84aec94c2742c92da1f19ce"
   },
   "source": [
    "\n",
    "\n",
    "# 9. Conclusion <a id=\"9\"></a> <br>\n",
    "\n",
    "**So by now I hope you had a fair understanding of how to do Hyperparameter Tuning with open source libraries as mentioned above.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
